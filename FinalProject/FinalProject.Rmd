---
title: 'Final Project'
author: 'Shovan Biswas'
date: '2020/12/04'
output:
# rmdformats::readthedown
  pdf_document:
    latex_engine: xelatex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Libraries          

```{r, eval=TRUE, message=FALSE, warning=FALSE}
library(tidyverse)
library(caret)
library(ROCR)
library(e1071)
library(pROC)
library(class)
library(knitr)
library(randomForest)
```

# PART A         

STEP#0: Pick any two classifiers of (SVM,Logistic,DecisionTree,NaiveBayes). Pick heart or ecoli dataset. Heart is simpler and ecoli compounds the problem as it is NOT a balanced dataset. From a grading perspective both carry the same weight.

STEP#1 For each classifier, Set a seed (43)        

STEP#2 Do a 80/20 split and determine the Accuracy, AUC and as many metrics as returned by the Caret package (confusionMatrix) Call this the base_metric. Note down as best as you can development (engineering) cost as well as computing cost(elapsed time).        

Start with the original dataset and set a seed (43). Then run a cross validation of 5 and 10 of the model on the training set. Determine the same set of metrics and compare the cv_metrics with the base_metric. Note down as best as you can development (engineering) cost as well as computing cost(elapsed time).        

Start with the original dataset and set a seed (43) Then run a bootstrap of 200 resamples and compute the same set of metrics and for each of the two classifiers build a three column table for each experiment (base, bootstrap, cross-validated). Note down as best as you can development (engineering) cost as well as computing cost(elapsed time).       


# Answer:                  

Before writing anything, I'll write all the necessary functions here. Since I have to repeat the same or similar logic in many of the models, I coded common functions at the top.         

I tailored the logic of the RMD programs, given in class to suite the current requirements.            

## Functions block          

```{r}
# Split dataset

split_dataset <- function(seed_value, dataset, prct) {

  set.seed(seed_value)

  split_index <- sample(1:nrow(dataset), prct * nrow(dataset), replace = F)
  train <- dataset[-split_index,]
  test <- dataset[split_index,]
  
  return(list(train, test))
}
```


```{r}

model_build <- function(model_name, train, type) {
  
  if (model_name == 'glm') {
    model <- glm(target~., data = train, family = 'binomial')
  } else if (model_name == 'nb') {
    model <- naiveBayes(target~., data = train)
  } else if (model_name == 'rf') {
    model <- randomForest(target~., data = train, ntree = type)
  }
  
  return(model)
}
```


```{r}
model_predict <- function(model_name, model, test, type) {
  
  if (model_name == 'glm') {
    
    model_prediction <- predict(model, newdata = test[,-c(14)], type = type)
    model_prediction_class <- ifelse(model_prediction < 0.5, 0, 1)     # Crux of Logistic Regression activity.
    model_caret_results <- caret::confusionMatrix(table(test[[14]], model_prediction_class))
    
  } else if (model_name == 'nb') {
    
    model_prediction <- predict(model, newdata = test[,-c(14)], type = type)
    model_prediction_class <- unlist(apply(round(model_prediction), 1, which.max)) - 1
    model_caret_results <- caret::confusionMatrix(table(test[[14]], model_prediction_class))
    
  } else if (model_name == 'rf') {
    
    model_prediction <- predict(model, newdata = test[,-c(14)])
    model_prediction_class <- table(test$target, model_prediction)
    model_caret_results <- caret::confusionMatrix(model_prediction_class)

  }
  
  return(list(model_caret_results, model_prediction))
}
```


```{r}
cv_folds_create <- function(train, NF) {
  
  # Splitting numbers from 1 to N (N is number of rows in file) into folds of size NF (5, 10 etc). At this point we are not touching the data in data_train_redo.        

  N <- nrow(train)
  NF = NF
  folds <- split(1:N, cut(1:N, quantile(1:N, probs = seq(0, 1, by = 1/NF))))  # Generates NF folds or buckets of number from 1 to N (i.e. from 1 to the number of rows in the file).
  
  return(folds)
}
```


```{r}
create_sample <- function(data_set, tf_value) {
  
  # The function **Sample(1:n, s, replace = F)** randomly selects s numbers from the vector 1:n. Following function sample randomly selects nrow(train) elements
  # from the vector 1:nrow(train). Note that the numbers are the same. So, it effectively randomizes the order of vectors in 1:nrow(data_train_redo) and produces
  # a new vector ridx. So, far we have not touched the actual data in train.          
  
  ridx <- sample(1:nrow(data_set), nrow(data_set), replace = tf_value)   # Randomize the data
  
  return(ridx)
}
```


```{r}

cv_model_build <- function(model_name, folds, train, type, ridx) {

  cv_df <- do.call('rbind', lapply(folds, FUN = function(idx, data = train[ridx,]) {
    
    if (model_name == 'glm') {
      m <- glm(target~., data = data[-idx,], family = 'binomial')
      p <- predict(m, data[idx, -c(14)], type = type)
      pc <- ifelse(p < 0.5, 0, 1)
    } else if (model_name == 'nb') {
      m <- naiveBayes(target~., data = data[-idx,])
      p <- predict(m, data[idx, -c(14)], type = type)
      pc <- unlist(apply(round(p), 1, which.max)) - 1
    }
    
    pred_tbl <- table(data[idx, c(14)], pc)
    pred_cfm <- caret::confusionMatrix(pred_tbl)
    list(fold = idx, m = m, cfm = pred_cfm)
  }
  ))
  
  return(cv_df)
}
```


```{r}

cv_model_predict <- function(model_name, cv_df, test, type) {

  tstcv_preds <- lapply(cv_df, FUN = function(M, D = test[,-c(14)]) predict(M, D, type = type))
  tstcv_cfm <- lapply(tstcv_preds, FUN = function(P, A = test[[14]])  {
    
    if (model_name == 'glm') {
      pred_class <- ifelse(P < 0.5, 0, 1)
    } else if (model_name == 'nb') {
      pred_class <- unlist(apply(round(P), 1, which.max)) - 1
    }

    pred_tbl <- table(pred_class, A)
    pred_cfm <- caret::confusionMatrix(pred_tbl)
  }
  )
  
  return(list(tstcv_cfm, tstcv_preds))
}
```


```{r}

cv_compute_param_average <- function(tstcv_cfm) {
  
  tstcv_perf <- as.data.frame(do.call('rbind', lapply(tstcv_cfm, FUN = function(cfm) c(cfm$overall, cfm$byClass))))
  cv_tst_perf <- apply(tstcv_perf[tstcv_perf$AccuracyPValue < 0.01, -c(6:7)], 2, mean)   # Compute Average of all the key parameters
  cv_tst_perf_df <- data.frame(cv_tst_perf)

  # cv_tst_perf_var <- apply(tstcv_perf[tstcv_perf$AccuracyPValue < 0.01, -c(6:7)], 2, sd)
    
  return(cv_tst_perf_df)  
}
```


```{r}

cv_compute_confusionmatrix_average <- function(tstcv_cfm) {

  tstcv_perf <- as.data.frame(do.call('rbind', lapply(tstcv_cfm, FUN = function(cfm) c(cfm$overall, cfm$table))))
  cv_tst_perf <- apply(tstcv_perf[tstcv_perf$AccuracyPValue < 0.01, -c(6:7)], 2, mean)
  cv_confusion_matrix <- matrix(c(cv_tst_perf[6], cv_tst_perf[7], cv_tst_perf[8], cv_tst_perf[9]), nrow = 2)
  
  return(cv_confusion_matrix)
}
```


```{r}

cv_compute_AUC_average <- function(model_name, tstcv_preds, NF) {
  
  if (model_name == 'glm') {
    tstcv_preds_df <- data.frame(tstcv_preds)
    sum <- rep(0, nrow(tstcv_preds_df))   # There are 60 items in each prediction
    for(i in 1:NF) {
      sum <- sum + tstcv_preds_df[i]
    }
  
  } else if (model_name == 'nb') {
    sum <- rep(0, nrow(data.frame(tstcv_preds)))   # There are 60 items in each prediction
    for(i in 1:NF) {
      sum <- sum + tstcv_preds[[i]][,2]
    }
  }
  
  cv_prediction <- sum / NF
  cv_auc <- performance(prediction(cv_prediction, data_test_redo$target), 'auc')@y.values[[1]]
  
  return(cv_auc)
}
```


## Terminology          

Accuracy or **Balanced Accuracy** $= \frac{TP + TN}{(TP + FP + FN + TN)}$      (https://en.wikipedia.org/wiki/Confusion_matrix)                        
Sensitivity or **Recall** or $TPR = $\frac{TP}{(TP + FN)}$                     (https://en.wikipedia.org/wiki/Confusion_matrix)                        
Specificity or $TNR = \frac{TN}{(TN + FP)}$                                    (https://en.wikipedia.org/wiki/Confusion_matrix)                        
Pos Pred Value or **Precision** or $PPV = \frac{TP}{(TP + FP)}$                (https://en.wikipedia.org/wiki/Confusion_matrix)                        
Neg Pred Value or $NPV = \frac{TN}{(TN + FN)}$                                 (https://en.wikipedia.org/wiki/Confusion_matrix)                        
Prevalence $= \frac{TP + FN}{(TP + FP + FN + TN)}$                             (https://en.wikipedia.org/wiki/Confusion_matrix)                        
Detection Rate $= \frac{TP}{(TP + FP + FN + TN)}$                              (https://stats.stackexchange.com/questions/316641/what-is-the-usefulness-of-detection-rate-in-a-confusion-matrix)      
Detection Prevalence $= \frac{TP + FP}{(TP + FP + FN + TN)}$                   (https://yardstick.tidymodels.org/reference/detection_prevalence.html)                        
F1 $= \frac{2TP}{(2TP + FP + FN)}$                                             (https://en.wikipedia.org/wiki/Confusion_matrix)                        




## Processing                

From the four given models, I am choosing **Logistic Regression** and **Naive Bayes**.       

## Loading Data                     

Since heart.csv was more often used and referred, I am using heart.csv. Furthermore, let me state that I'll mostly reuse the R code given in class.        

```{r}
heart <- read.csv("./heart.csv", header = T, sep = ",", stringsAsFactors = F)
names(heart)
```

As we see, the name of column age is displayed as **i..age**. I verified with UNIX command **cat -tve heart.csv** that this is due to the presence of control characters **'/M-oM-;M-?'** at the begining of heart.csv.         

I will remove control characters with UNIX command in the below code chunk. Note {bash} as opposed to {r} in below code chunk, which enables me to execute Unix commands from Cygwin installed on my computer. I set up my Windows and R to execute Unix shell scripts from RMD.           

In Professor Raman's RMD it was handled a bit differently, by changing the column name age, with **names(heart)[[1]] <- 'age'**, but I experimented a different approach here, because I don't like to keep control characters in a dataset.        

```{bash}
cat -tve heart.csv | sed 's/M-oM-;M-?//g' | sed 's/...$//' > heart2.csv
```



The real processing of basic Logistic Regression and Naive Bayes begins here. So, I'll start counting time from this point. But, some tasks are common, so I'll compute it separately.         

```{r}
ptm <- proc.time()  # timing the common parts, start
```

```{r}
heart <- read.csv("./heart2.csv", header = T, sep = ",", stringsAsFactors = F)
names(heart)
```

```{r}
names(heart)[[1]] <- 'age'
heart$target <- as.factor(heart$target)
```

```{r}
dim(heart)
```

Checking for constants in all rows of each column.           

```{r}
isConstant <- function(x) length(names(table(x))) < 2 
apply(heart, 2, isConstant) 
```

```{r}
classLabels <- table(heart$target)
print(classLabels)
```

```{r}
ifelse(length(names(classLabels)) == 2, "binary classification", "multi-class classification")
```

So, we see that there are two values, 0 and 1 in heart$target. So, it's a case binary classification.       





## Splitting data                     

Now, we'll split the data into train and test.

```{r}
split_data <- split_dataset(43, heart, 0.20)

data_train <- as.data.frame(split_data[1])
data_test <- as.data.frame(split_data[2])
```

```{r}
common_execution_tm <- proc.time() - ptm  # timing the common parts, end
```

At this point, the common part is over.             


## Running Logistic Regression Model                 

```{r}
ptm <- proc.time()  # timing the Logistic Regression parts, start

model_name <- 'glm'
type_name <- 'response'
```


```{r}

model <- model_build(model_name, data_train, type_name)

model_prediction <- model_predict(model_name, model, data_test, type_name)
model_confusionmatrix <- model_prediction[1][[1]][2]
model_accuracy <- data.frame(model_prediction[1][[1]][3])[1, 1]

model_caret_results_df <- as.data.frame(model_prediction[1][[1]][4])

model_auc <- performance(prediction(model_prediction[2][1], data_test$target), 'auc')@y.values[[1]]
```

Summary of Logistic Regression Model.       
```{r}
summary(model)
```

Confusion Matrix.        
```{r}
model_confusionmatrix
```

Accuracy.        
```{r}
model_accuracy
```

AUC.      
```{r}
model_auc
```

Below figure displays the terms **Balanced Accuracy**, **Precision**, **Recall**, which are the same as **Accuracy**, **Pos Pred Value**, **Sensitivity** respectively. So, in order not to count twice, I'll exclude the terms **Precision**, **Recall**, **Balanced Accuracy** from the basic metric. There is an additional term F1, which I'll include in the basic metric. I explained the terms in Terminology section above.         

```{r}
model_caret_results_df
```


Execution time.              

```{r}
lr_execution_tm <- proc.time() - ptm  # timing the Logistic Regression parts, end

tot_tm <- lr_execution_tm + common_execution_tm

tot_tm
```

I'll collect **AUC**, **Accuracy**, **Sensitivity**, **Specificity**, **Pos Pred Value**, **Neg Pred Value**, **Prevalence**, **Detection Rate**, **Detection Prevalence** and an additional term **F1** to build basic metric vector for Logistic Regression. Additionally, I'll include the computation time for this model.        


```{r}
lr_basic_metric <- c('Logistic Regression', model_auc, model_accuracy, model_caret_results_df[1, 1], model_caret_results_df[2, 1], model_caret_results_df[3, 1], model_caret_results_df[4, 1], model_caret_results_df[7, 1], model_caret_results_df[8, 1], model_caret_results_df[9, 1], model_caret_results_df[10, 1], data.frame(tot_tm[3])[1,])
```








## Running Naive Bayes Model                

```{r}
ptm <- proc.time()  # timing the Logistic Naive Bayes, start

model_name <- 'nb'
type_name <- 'raw'
```

```{r}
model <- model_build(model_name, data_train, type_name)

model_prediction <- model_predict(model_name, model, data_test, type_name)
model_confusionmatrix <- model_prediction[1][[1]][2]
model_accuracy <- data.frame(model_prediction[1][[1]][3])[1, 1]

model_caret_results_df <- as.data.frame(model_prediction[1][[1]][4])

model_auc <- performance(prediction(model_prediction[[2]][,2], data_test$target), 'auc')@y.values[[1]]
# model_auc <- performance(prediction(data.frame(model_prediction[[2]][,2]), data_test$target), 'auc')@y.values[[1]]
```

Summary of Naive Bayes Model.      
```{r}
summary(model)
```

Confusion Matrix.     
```{r}
model_confusionmatrix
```

Accuracy.       
```{r}
model_accuracy
```

AUC
```{r}
model_auc
```

All key parameters.        

```{r}
model_caret_results_df
```

Execution time.      

```{r}
nb_execution_tm <- proc.time() - ptm  # timing the Logistic Naive Bayes, end

tot_tm <- nb_execution_tm + common_execution_tm

tot_tm
```


Now, I'll build the basic metric vector for Naive Bayes.        

```{r}
nb_basic_metric <- c('Naive Bayes', model_auc, model_accuracy, model_caret_results_df[1, 1], model_caret_results_df[2, 1], model_caret_results_df[3, 1], model_caret_results_df[4, 1], model_caret_results_df[7, 1], model_caret_results_df[8, 1], model_caret_results_df[9, 1], model_caret_results_df[10, 1], data.frame(tot_tm[3])[1,])
```

## Summary table for Logistic Regression and Naive Bayes.      

```{r}
metric_table <- data.frame(matrix(ncol = 12, nrow = 0))

metric_table <- rbind(metric_table, lr_basic_metric, nb_basic_metric)

colnames(metric_table) <- c("Model", "AUC", "Accuracy", "Sensitivity", "Specificity", "Pos Pred Value", "Neg Pred Value", "Prevalence", "Detection Rate", "Detection Prevalence", "F1", "Elapsed time")


metric_table %>% kable()
```





## Cross validation                    

## Splitting data.          

In Cross Validation, begins here. The common portions of CV are being recorded first.         

```{r}
ptm <- proc.time()  # timing the CV common part, start
```


```{r}
split_data <- split_dataset(43, heart, 0.20)

data_train_redo <- as.data.frame(split_data[1])
data_test_redo <- as.data.frame(split_data[2])
```

```{r}
cv_common_execution_tm <- proc.time() - ptm  # timing the CV common part, end
```






## Cross validation with Logistic Regression Model with folds = 10.         

Please be informed that I used code from lecture M09 and modified it wherever required.        

```{r}
ptm <- proc.time()  # timing the CV Logistic Regression with 10 folds part, start

model_name <- 'glm'
type_name <- 'response'
```

I'll split the numbers from 1 to N (N is number of rows in the file) into folds or buckets of size NF = 10 and build glm model.        

```{r}
NF = 10
folds <- cv_folds_create(data_train_redo, NF)

ridx <- create_sample(data_train_redo, FALSE)
cv_df <- data.frame(cv_model_build(model_name, folds, data_train_redo, type_name, ridx))   # Build glm model



cv_model_prediction <- cv_model_predict(model_name, cv_df$m, data_test_redo, type_name)
tstcv_cfm <- cv_model_prediction[1][[1]]
tstcv_preds <- cv_model_prediction[2][[1]]


# Average of all key parameters (Accuracy etc) over 10 folds is being computed below.        
cv_tst_perf_df <- cv_compute_param_average(tstcv_cfm)                     # Average of all key parameters
cv_confusion_matrix <- cv_compute_confusionmatrix_average(tstcv_cfm)      # Average confusion matrix.       
cv_auc <- cv_compute_AUC_average(model_name, tstcv_preds, NF)             # Average AUC.
```

Confusion Matrix.        

```{r}
cv_confusion_matrix
```

Observe fractional values at the cells of confusion matrix, due to averaging over the folders.        


AUC

```{r}
cv_auc
```

Average of all key parameters.        

```{r}
cv_tst_perf_df
```

Execution time.       

```{r}
cv_lr_10_execution_tm <- proc.time() - ptm  # timing the CV Logistic Regression with 10 folds part, end

tot_tm <- cv_common_execution_tm + cv_lr_10_execution_tm

tot_tm
```


Now, I'll build the metric vector for this Cross Validation.        

```{r}
cv_lr_10_metric <- c('Cross Validation of LR with 10 folds', cv_auc, cv_tst_perf_df[1, 1], cv_tst_perf_df[6, 1], cv_tst_perf_df[7, 1], cv_tst_perf_df[8, 1], cv_tst_perf_df[9, 1], cv_tst_perf_df[12, 1], cv_tst_perf_df[13, 1], cv_tst_perf_df[14, 1], cv_tst_perf_df[15, 1], data.frame(tot_tm[3])[1,])
```



## Cross validation with Logistic Regression Model with folds = 5.          

Please be informed that I used code from lecture M09 and modified it wherever required.        

```{r}
ptm <- proc.time()     # timing the CV Logistic Regression with 5 folds part, start
```

I'll split the numbers from 1 to N (N is number of rows in the file) into folds or buckets of size NF = 5 and build glm model.         

```{r}
NF = 5
folds <- cv_folds_create(data_train_redo, NF)

ridx <- create_sample(data_train_redo, FALSE)
cv_df <- as.data.frame(cv_model_build(model_name, folds, data_train_redo, type_name, ridx))   # Build glm model


cv_model_prediction <- cv_model_predict(model_name, cv_df$m, data_test_redo, type_name)
tstcv_cfm <- cv_model_prediction[1][[1]]
tstcv_preds <- cv_model_prediction[2][[1]]


# Average of all key parameters (Accuracy etc) over 5 folds is being computed below.        
cv_tst_perf_df <- cv_compute_param_average(tstcv_cfm)                     # Average of all key parameters
cv_confusion_matrix <- cv_compute_confusionmatrix_average(tstcv_cfm)      # Average confusion matrix.       
cv_auc <- cv_compute_AUC_average(model_name, tstcv_preds, NF)             # Average AUC.
```


Confusion Matrix.        

```{r}
cv_confusion_matrix
```

Observe fractional values at the cells of confusion matrix, due to averaging over the folders.        


AUC

```{r}
cv_auc
```


Average of all key parameters.        

```{r}
cv_tst_perf_df
```

Execution time.       

```{r}
cv_lr_5_execution_tm <- proc.time() - ptm     # timing the CV Logistic Regression with 5 folds part, end

tot_tm <- cv_common_execution_tm + cv_lr_5_execution_tm

tot_tm
```


Now, I'll build the metric vector for this Cross Validation.        

```{r}
cv_lr_5_metric <- c('Cross Validation of LR with 5 folds', cv_auc, cv_tst_perf_df[1, 1], cv_tst_perf_df[6, 1], cv_tst_perf_df[7, 1], cv_tst_perf_df[8, 1], cv_tst_perf_df[9, 1], cv_tst_perf_df[12, 1], cv_tst_perf_df[13, 1], cv_tst_perf_df[14, 1], cv_tst_perf_df[15, 1], data.frame(tot_tm[3])[1,])
```

## Cross validation with Naive Bayes Model with folds = 10.           

```{r}
ptm <- proc.time()     # timing the CV Naive Bayes with 10 folds part, start

model_name <- 'nb'
type_name <- 'raw'
```

I'll split the numbers from 1 to N (N is number of rows in the file) into folds or buckets of size NF = 10 and build the NB model.              

```{r}
NF = 10
folds <- cv_folds_create(data_train_redo, NF)

ridx <- create_sample(data_train_redo, FALSE)
cv_df <- as.data.frame(cv_model_build(model_name, folds, data_train_redo, type_name, ridx))   # Build glm model


cv_model_prediction <- cv_model_predict(model_name, cv_df$m, data_test_redo, type_name)
tstcv_cfm <- cv_model_prediction[1][[1]]
tstcv_preds <- cv_model_prediction[2][[1]]


# Average of all key parameters (Accuracy etc) over 10 folds is being computed below.        
cv_tst_perf_df <- cv_compute_param_average(tstcv_cfm)                     # Average of all key parameters
cv_confusion_matrix <- cv_compute_confusionmatrix_average(tstcv_cfm)      # Average confusion matrix.       
cv_auc <- cv_compute_AUC_average(model_name, tstcv_preds, NF)             # Average AUC.
```


Confusion Matrix.        

```{r}
cv_confusion_matrix
```

Observe fractional values at the cells of confusion matrix, due to averaging over the folders.        


AUC

```{r}
cv_auc
```

Average of all key parameters.        

```{r}
cv_tst_perf_df
```

Execution time.      

```{r}
cv_nb_10_execution_tm <- proc.time() - ptm     # timing the CV Naive Bayes with 10 folds part, end

tot_tm <- cv_common_execution_tm + cv_nb_10_execution_tm

tot_tm
```



Now, I'll build the metric vector for this Cross Validation.        

```{r}
cv_nb_10_metric <- c('Cross Validation of NB with 10 folds', cv_auc, cv_tst_perf_df[1, 1], cv_tst_perf_df[6, 1], cv_tst_perf_df[7, 1], cv_tst_perf_df[8, 1], cv_tst_perf_df[9, 1], cv_tst_perf_df[12, 1], cv_tst_perf_df[13, 1], cv_tst_perf_df[14, 1], cv_tst_perf_df[15, 1], data.frame(tot_tm[3])[1,])
```



## Cross validation with Naive Bayes Model with folds = 5.           

```{r}
ptm <- proc.time()     # timing the CV Naive Bayes with 5 folds part, start
```

I'll split the numbers from 1 to N (N is number of rows in the file) into folds or buckets of size NF = 5 and build NB model.            

```{r}
NF = 5
folds <- cv_folds_create(data_train_redo, NF)

ridx <- create_sample(data_train_redo, FALSE)
cv_df <- as.data.frame(cv_model_build(model_name, folds, data_train_redo, type_name, ridx))   # Build glm model


cv_model_prediction <- cv_model_predict(model_name, cv_df$m, data_test_redo, type_name)
tstcv_cfm <- cv_model_prediction[1][[1]]
tstcv_preds <- cv_model_prediction[2][[1]]


# Average of all key parameters (Accuracy etc) over 5 folds is being computed below.        
cv_tst_perf_df <- cv_compute_param_average(tstcv_cfm)                     # Average of all key parameters
cv_confusion_matrix <- cv_compute_confusionmatrix_average(tstcv_cfm)      # Average confusion matrix.       
cv_auc <- cv_compute_AUC_average(model_name, tstcv_preds, NF)             # Average AUC.
```


Confusion Matrix.        

```{r}
cv_confusion_matrix
```

Observe fractional values at the cells of confusion matrix, due to averaging over the folders.        


AUC

```{r}
cv_auc
```

Average of all key parameters.        

```{r}
cv_tst_perf_df
```


Execution time.

```{r}
cv_nb_5_execution_tm <- proc.time() - ptm     # timing the CV Naive Bayes with 5 folds part, end

tot_tm <- cv_common_execution_tm + cv_nb_5_execution_tm

tot_tm
```



Now, I'll build the basic metric vector for Cross Validation.        

```{r}
cv_nb_5_metric <- c('Cross Validation of NB with 5 folds', cv_auc, cv_tst_perf_df[1, 1], cv_tst_perf_df[6, 1], cv_tst_perf_df[7, 1], cv_tst_perf_df[8, 1], cv_tst_perf_df[9, 1], cv_tst_perf_df[12, 1], cv_tst_perf_df[13, 1], cv_tst_perf_df[14, 1], cv_tst_perf_df[15, 1], data.frame(tot_tm[3])[1,])
```

## Summary table for Cross Validation.      

```{r}
metric_table <- data.frame(matrix(ncol = 12, nrow = 0))

metric_table <- rbind(metric_table, cv_lr_10_metric, cv_lr_5_metric, cv_nb_10_metric, cv_nb_5_metric)

colnames(metric_table) <- c("Model", "AUC", "Accuracy", "Sensitivity", "Specificity", "Pos Pred Value", "Neg Pred Value", "Prevalence", "Detection Rate", "Detection Prevalence", "F1", "Elapsed time")


metric_table %>% kable()
```







## Bootstrapping.           

```{r}
ptm <- proc.time()  # timing the BS part, start
```

```{r}
split_data <- split_dataset(43, heart, 0.20)

data_train_redo <- as.data.frame(split_data[1])
data_test_redo <- as.data.frame(split_data[2])
```

```{r}
bs_common_execution_tm <- proc.time() - ptm  # timing the BS part, end
```


## Bootstrapping with Logistic Regression Model.           

```{r}
ptm <- proc.time()  # timing the BS with Logistic Regression, start

model_name <- 'glm'
type_name <- 'response'
```

```{r}
ridx <- create_sample(data_train_redo, TRUE)

model <- model_build(model_name, data_train_redo[ridx,], type_name)
runModel <- function(data_train_redo) { model }
lapplyrunmodel <- function(x)runModel(data_train_redo)

NF = 200
model <- lapply(1:NF,lapplyrunmodel)
```


```{r}
cv_model_prediction <- cv_model_predict(model_name, model, data_test_redo, type_name)
tstcv_cfm <- cv_model_prediction[1][[1]]
tstcv_preds <- cv_model_prediction[2][[1]]


# Average of all key parameters (Accuracy etc) over 10 folds is being computed below.        
cv_tst_perf_df <- cv_compute_param_average(tstcv_cfm)                     # Average of all key parameters
cv_confusion_matrix <- cv_compute_confusionmatrix_average(tstcv_cfm)      # Average confusion matrix.       
cv_auc <- cv_compute_AUC_average(model_name, tstcv_preds, NF)             # Average AUC.
```


Confusion Matrix.        

```{r}
cv_confusion_matrix
```

AUC

```{r}
cv_auc
```


Average of all key parameters.        

```{r}
cv_tst_perf_df
```

Execution time.

```{r}
bs_lr_execution_tm <- proc.time() - ptm    # timing the BS with Logistic Regression, end

tot_tm <- bs_common_execution_tm + bs_lr_execution_tm

tot_tm
```


Now, I'll build the metric vector for this Bootstrapping.        

```{r}
bs_lr_metric <- c('Bootstrapping with LR', cv_auc, cv_tst_perf_df[1, 1], cv_tst_perf_df[6, 1], cv_tst_perf_df[7, 1], cv_tst_perf_df[8, 1], cv_tst_perf_df[9, 1], cv_tst_perf_df[12, 1], cv_tst_perf_df[13, 1], cv_tst_perf_df[14, 1], cv_tst_perf_df[15, 1], data.frame(tot_tm[3])[1,])

bs_lr_metric
```


## Bootstrapping with Naive Bayes Model.           

```{r}
ptm <- proc.time()  # timing the BS with Naive Bayes, start

model_name <- 'nb'
type_name <- 'raw'
```


```{r}
ridx <- create_sample(data_train_redo, TRUE)

model <- model_build(model_name, data_train_redo[ridx,], type_name)
runModel <- function(data_train_redo) { model }
lapplyrunmodel <- function(x)runModel(data_train_redo)

NF = 200
model <- lapply(1:NF,lapplyrunmodel)
```




```{r}
cv_model_prediction <- cv_model_predict(model_name, model, data_test_redo, type_name)
tstcv_cfm <- cv_model_prediction[1][[1]]
tstcv_preds <- cv_model_prediction[2][[1]]


# Average of all key parameters (Accuracy etc) over 10 folds is being computed below.        
cv_tst_perf_df <- cv_compute_param_average(tstcv_cfm)                     # Average of all key parameters
cv_confusion_matrix <- cv_compute_confusionmatrix_average(tstcv_cfm)      # Average confusion matrix.       
cv_auc <- cv_compute_AUC_average(model_name, tstcv_preds, NF)             # Average AUC.
```


Confusion Matrix.        

```{r}
cv_confusion_matrix
```

AUC

```{r}
cv_auc
```

Average of all key parameters.        

```{r}
cv_tst_perf_df
```

Now, I'll build the metric vector for this Bootstrapping.        

Execution time.         

```{r}
bs_nb_execution_tm <- proc.time() - ptm    # timing the BS with Logistic Regression, end

tot_tm <- bs_common_execution_tm + bs_nb_execution_tm

tot_tm
```



```{r}
bs_nb_metric <- c('Bootstrapping with NB', cv_auc, cv_tst_perf_df[1, 1], cv_tst_perf_df[6, 1], cv_tst_perf_df[7, 1], cv_tst_perf_df[8, 1], cv_tst_perf_df[9, 1], cv_tst_perf_df[12, 1], cv_tst_perf_df[13, 1], cv_tst_perf_df[14, 1], cv_tst_perf_df[15, 1], data.frame(tot_tm[3])[1,])

bs_nb_metric
```


## Summary table for Bootstrapping.        

```{r}
metric_table <- data.frame(matrix(ncol = 12, nrow = 0))

metric_table <- rbind(metric_table, bs_lr_metric, bs_nb_metric)

colnames(metric_table) <- c("Model", "AUC", "Accuracy", "Sensitivity", "Specificity", "Pos Pred Value", "Neg Pred Value", "Prevalence", "Detection Rate", "Detection Prevalence", "F1", "Elapsed time")


metric_table %>% kable()
```


## Summary table for Part A.        

```{r}
metric_table <- data.frame(matrix(ncol = 12, nrow = 0))

metric_table <- rbind(metric_table, lr_basic_metric, nb_basic_metric, cv_lr_10_metric, cv_lr_5_metric, cv_nb_10_metric, cv_nb_5_metric, bs_lr_metric, bs_nb_metric)

colnames(metric_table) <- c("Model", "AUC", "Accuracy", "Sensitivity", "Specificity", "Pos Pred Value", "Neg Pred Value", "Prevalence", "Detection Rate", "Detection Prevalence", "F1", "Elapsed time")


metric_table %>% kable()
```



# PART B         

For the same dataset, set seed (43) split 80/20.        

Using randomForest grow three different forests varying the number of trees atleast three times. Start with seeding and fresh split for each forest. Note down as best as you can development (engineering) cost as well as computing cost(elapsed time) for each run. And compare these results with the experiment in Part A. Submit a pdf and executable script in python or R.         


# Answer:                  

## First Random Forest execution.

I'll run first RF with 40.       

```{r}
ptm <- proc.time()  # timing the RF with 40, start
model_name  <- 'rf'
```

```{r}
split_data <- split_dataset(43, heart, 0.20)

data_train_redo <- as.data.frame(split_data[1])
data_test_redo <- as.data.frame(split_data[2])
```

```{r}
model <- model_build(model_name, data_train_redo, 40)

model_prediction <- model_predict(model_name, model, data_test_redo, 'NO')
model_confusionmatrix <- model_prediction[1][[1]][2]
model_accuracy <- data.frame(model_prediction[1][[1]][3])[1, 1]
model_caret_results_df <- as.data.frame(model_prediction[1][[1]][4])

# Note: My usual method for computing auc will not work in this case, because the the variable model_prediction_class is just a 2 x 2 matrix.       
auc <- roc(as.numeric(data_test_redo$target), as.numeric(as.matrix((predict(model, data_test_redo, type = "prob")[,2]))))$auc
```

Summary of Logistic Regression Model.       
```{r}
summary(model)
```

Confusion Matrix.        
```{r}
model_confusionmatrix
```

Accuracy.        
```{r}
model_accuracy
```

AUC.      
```{r}
model_auc
```

Average of all key parameters.        

```{r}
model_caret_results_df
```

Execution time.      

```{r}
rf_40_execution_tm <- proc.time() - ptm  # timing the RF with 40, end

tot_tm <- rf_40_execution_tm

tot_tm
```


Now, I'll build the metric vector for this Random Forest with 40.        


```{r}
rf_40_metric <- c('Random Forest with 40', model_auc, model_accuracy, model_caret_results_df[1, 1], model_caret_results_df[2, 1], model_caret_results_df[3, 1], model_caret_results_df[4, 1], model_caret_results_df[7, 1], model_caret_results_df[8, 1], model_caret_results_df[9, 1], model_caret_results_df[10, 1], data.frame(tot_tm[3])[1,])
```




## Second Random Forest execution.

I'll grow by RF by 60.       

```{r}
ptm <- proc.time()  # timing the RF with 40, start
```

```{r}
split_data <- split_dataset(43, heart, 0.20)

data_train_redo <- as.data.frame(split_data[1])
data_test_redo <- as.data.frame(split_data[2])
```


WHen I used model returned by the first run in grow(model, 60), I got the following error:           
"Error in terms.formula(formula, data = data) : 'data' argument is of the wrong type"                

However, on directly calling randomForest, without calling via my function model_build, I did not get the same error. It's a bit weird because the the model returned by first run was used for predicting, and I also checked the class of model with model(class) and got the right results "[1] "randomForest.formula" "randomForest"".     

So, now that I am out of time and also lost quite a bit in other trouble shootings, I am directly calling randomForest, as a quick and easy fix.       

```{r}
# class(model)
model <- randomForest(target~., data = data_train_redo, ntree = 40)
```

```{r}
model <- grow(model, 60)
model_prediction <- model_predict(model_name, model, data_test_redo, 'NO')
model_confusionmatrix <- model_prediction[1][[1]][2]
model_accuracy <- data.frame(model_prediction[1][[1]][3])[1, 1]
model_caret_results_df <- as.data.frame(model_prediction[1][[1]][4])

# Note: My usual method for computing auc will not work in this case, because the the variable model_prediction_class is just a 2 x 2 matrix.       
auc <- roc(as.numeric(data_test_redo$target), as.numeric(as.matrix((predict(model, data_test_redo, type = "prob")[,2]))))$auc
```

Summary of Logistic Regression Model.       
```{r}
summary(model)
```

Confusion Matrix.        
```{r}
model_confusionmatrix
```

Accuracy.        
```{r}
model_accuracy
```

AUC.      
```{r}
model_auc
```

Average of all key parameters.        

```{r}
model_caret_results_df
```

Execution time.      

```{r}
rf_60_execution_tm <- proc.time() - ptm  # timing the RF with 40, end

tot_tm <- rf_60_execution_tm

tot_tm
```


Now, I'll build the metric vector for this Random Forest with 100.        

```{r}
rf_60_metric <- c('Random Forest with 100', model_auc, model_accuracy, model_caret_results_df[1, 1], model_caret_results_df[2, 1], model_caret_results_df[3, 1], model_caret_results_df[4, 1], model_caret_results_df[7, 1], model_caret_results_df[8, 1], model_caret_results_df[9, 1], model_caret_results_df[10, 1], data.frame(tot_tm[3])[1,])
```




## Third Random Forest execution.     

I'll grow by RF by 100.       

```{r}
ptm <- proc.time()  # timing the RF with 40, start
```

```{r}
split_data <- split_dataset(43, heart, 0.20)

data_train_redo <- as.data.frame(split_data[1])
data_test_redo <- as.data.frame(split_data[2])
```


```{r}
model <- grow(model, 100)
model_prediction <- model_predict(model_name, model, data_test_redo, 'NO')
model_confusionmatrix <- model_prediction[1][[1]][2]
model_accuracy <- data.frame(model_prediction[1][[1]][3])[1, 1]
model_caret_results_df <- as.data.frame(model_prediction[1][[1]][4])

# Note: My usual method for computing auc will not work in this case, because the the variable model_prediction_class is just a 2 x 2 matrix.       
auc <- roc(as.numeric(data_test_redo$target), as.numeric(as.matrix((predict(model, data_test_redo, type = "prob")[,2]))))$auc
```

Summary of Logistic Regression Model.       
```{r}
summary(model)
```

Confusion Matrix.        
```{r}
model_confusionmatrix
```

Accuracy.        
```{r}
model_accuracy
```

AUC.      
```{r}
model_auc
```

Average of all key parameters.        

```{r}
model_caret_results_df
```

Execution time.       

```{r}
rf_100_execution_tm <- proc.time() - ptm  # timing the RF with 40, end

tot_tm <- rf_100_execution_tm

tot_tm
```

Now, I'll build the metric vector for this Random Forest with 160.        

```{r}
rf_100_metric <- c('Random Forest with 160', model_auc, model_accuracy, model_caret_results_df[1, 1], model_caret_results_df[2, 1], model_caret_results_df[3, 1], model_caret_results_df[4, 1], model_caret_results_df[7, 1], model_caret_results_df[8, 1], model_caret_results_df[9, 1], model_caret_results_df[10, 1], data.frame(tot_tm[3])[1,])
```


## Summary table for Part B.        

```{r}
metric_table <- data.frame(matrix(ncol = 12, nrow = 0))

metric_table <- rbind(metric_table, rf_40_metric, rf_60_metric, rf_100_metric)

colnames(metric_table) <- c("Model", "AUC", "Accuracy", "Sensitivity", "Specificity", "Pos Pred Value", "Neg Pred Value", "Prevalence", "Detection Rate", "Detection Prevalence", "F1", "Elapsed time")


metric_table %>% kable()
```

# Part C

Include a summary of your findings. Which of the two methods bootstrap vs cv do you recommend to your customer? And why? Be elaborate. Including computing costs, engineering costs and model performance. Did you incorporate Pareto's maxim or the Razor and how did these two heuristics influence your decision?

# Answer:                  

## Summary table for Part A, B (all models).        


```{r}
metric_table <- data.frame(matrix(ncol = 12, nrow = 0))

metric_table <- rbind(metric_table, lr_basic_metric, nb_basic_metric, cv_lr_10_metric, cv_lr_5_metric, cv_nb_10_metric, cv_nb_5_metric, bs_lr_metric, bs_nb_metric, rf_40_metric, rf_60_metric, rf_100_metric)

colnames(metric_table) <- c("Model", "AUC", "Accuracy", "Sensitivity", "Specificity", "Pos Pred Value", "Neg Pred Value", "Prevalence", "Detection Rate", "Detection Prevalence", "F1", "Elapsed time")


metric_table %>% kable()
```


The table speaks for the performance of the model. Here ar some of the observations:      

1)Although the accuracies were the same, NB (2nd row) took a little more than LR (1st row). However, the AUC of NB was lesser than LR.     
2)With almost the same accuracies, each of the Cross Validations took much less time than standalone LR and NB.       
3)Bootstrapping took way more time, with almost same level of accuracy.
4)Random Forest seems to have performed better than others.

As far as I know, Pareto's principle states, "80% of consequences come from 20% of the causes". I am not sure how I would even apply this principle here.

Occam's Razor states that "Entities are not to multiplied without necessity" (Reference, History Of Western Philosophy, page 462, chapter XIV, "Franciscan Schoolmen"). The general interpretation of this has been, to reduce the number of assumtions to a minimum. In order to explain something, if it is sufficient to make three assumptions, then it's not necessary to postulate a fourth one.

Yes, I didn't make any unnecessary assumptions here.       

All in all, Random Forest seems to be the most performant, or did I make any mistake?       


**The Engineering**

In order to accomplish this project, since time is short, I selected two models. Furthermore, in doing part B, I am required to do Random Forest. So, Logistic Regression and Naive Bayes are not bad choices.       

In this exercise, I realized that there were repeatitiion of tasks. So, I wrote functions instead of flat out scripting approach. That way, I not only resued the same code over and over, but also built my own library of useful functions for later use. I saved time for future.

It was quite an arduous task. Had it been Python, I wouldn't have that many surprises. R doesn't return multiple values (not uncommon in languages). However, several objects can be put into a list to return out of a function. But, the return values were often not what I expected. So, many experiments had to be done and I had to constantly troubleshoot my way through a forest of quagmires. I enjoyed that.

Marker: 622_p
