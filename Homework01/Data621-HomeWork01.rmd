---
title: "DATA 622: Homework 01"
author: "Shovan Biswas"
date: "10/15/2020"
output:
  pdf_document: default
  html_document:
    highlight: pygments
    theme: cerulean
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(caret)
library(ROCR)
library(e1071)
library(pROC)
library(class)
library(knitr)
```

# Load input data dataset.csv
```{r MainCode}
mydata <- read.csv('./dataset.csv', head = TRUE, sep = ',', stringsAsFactors = TRUE)
head(mydata)
```

Converting variable X to factor.  
```{r}
mydata$X <- factor(mydata$X)
```

# Data exploration.  
```{r}
print(paste0("Number of observations: ", dim(mydata)[1], "   Number of columns: ", dim(mydata)[2])) 
```

Ratio of BLACK to BLUE in response variable label. The data is somewhat imbalanced.  
```{r}
table(mydata$label)
```

```{r}
summary(mydata)
```

```{r}
xtabs(~label + X, data = mydata)
```

```{r}
xtabs(~label + Y, data = mydata)
```

# Logistic Regression on entire dataset.  

## Executing Logistic Regression model i.e. glm() function.  
```{r}
lr_glm_model <- glm(label~., data = mydata, family = "binomial")
lr_glm_model
```


```{r}
summary(lr_glm_model)
```

## Now let us compute the performance of the classifier. We know that random choice would have scored 0.5. If our model is any good it must perform better than 0.5.  

## Prediction and generation of Confusion Matrix.  
```{r}
lr_prediction <- predict(lr_glm_model, newdata = mydata[,1:2], type = 'response')
lr_prediction_labels <- ifelse(lr_prediction > 0.5, "BLUE", "BLACK")
table(lr_prediction_labels)
```


```{r}
table(mydata$label)
```

## Computation of confusion matrix, with table() function.  
```{r}
lr_confusion_matrix <- table(mydata$label, lr_prediction_labels)

# Gathering parts of confusion matrix, for later use:
lr_TP = lr_confusion_matrix[1, 1]
lr_FP = lr_confusion_matrix[1, 2]
lr_FN = lr_confusion_matrix[2, 1]
lr_TN = lr_confusion_matrix[2, 2]

lr_confusion_matrix
```



## Computation of Accuracy parameter from confusion matrix.  
```{r}
lr_accuracy <- sum(diag(lr_confusion_matrix)) / sum(lr_confusion_matrix) * 100
lr_accuracy
```

## Verifying computation of confusion matrix, with caret::confusionMatrix function.  
```{r}
caret::confusionMatrix(table(mydata$label, lr_prediction_labels))
```



So, with both methods (i.e. with table() function and caret::confusionMatrix() function) Accuracy is 0.9167 = 91.67%, which is greater than 70%.  

**A model with a accuracy of 70% or higher is perofrmant, and therefore is not underfitting. So, we conclude that our model is capable of learning.**  

# ============== Having computed for the entire dataset, now we'll proceed to split the data and do necessary computations.  

# Split Data.  
```{r}
set.seed(43)
mydata_train_index <- sample(1:nrow(mydata), 0.30 * nrow(mydata), replace = F)
mydata_train <- mydata[-mydata_train_index, ]
mydata_test <- mydata[mydata_train_index, ]
```


Ratio of BLACK to BLUE in response variable label in train data. The data is imbalanced.    
```{r}
table(mydata_train$label)
```

Ratio of BLACK to BLUE in response variable label in test data. The data is almost balanced.    
```{r}
table(mydata_test$label)
```

# Logistic Regression on training dataset.  
```{r}
lr_glm_model_train <- glm(label~., data = mydata_train, family = 'binomial')
lr_glm_model_train
```

```{r}
summary(lr_glm_model_train)
```

```{r}
lr_prediction_train <- predict(lr_glm_model_train, newdata = mydata_train[,1:2], type = 'response')
lr_prediction_labels_train <- ifelse(lr_prediction_train > 0.5, "BLUE", "BLACK")
table(lr_prediction_labels_train)
```

```{r}
table(mydata_train$label)
```

## Computation of confusion matrix for train data, with table() function.  
```{r}
lr_confusion_matrix_train <- table(mydata_train$label, lr_prediction_labels_train)

# Gathering parts of confusion matrix, for later use:
lr_TP_train = lr_confusion_matrix_train[1, 1]
lr_FP_train = lr_confusion_matrix_train[1, 2]
lr_FN_train = lr_confusion_matrix_train[2, 1]
lr_TN_train = lr_confusion_matrix_train[2, 2]

lr_confusion_matrix_train
```



## Computation of Accuracy parameter from confusion matrix for train data.  
```{r}
lr_accuracy_train <- sum(diag(lr_confusion_matrix_train)) / sum(lr_confusion_matrix_train) * 100
lr_accuracy_train
```

## Verifying computation of confusion matrix for train data, with caret::confusionMatrix function.  
```{r}
caret::confusionMatrix(table(mydata_train$label, lr_prediction_labels_train))
```

**So, with both methods Accuracy is 100%, the model is able to learn.**  




Now, I'll compile together all the required information for train data i.e. AUC, ACCURACY, TPR, FPR, TNR, FNR.  

We have already computed Accuracy in lr_accuracy_train.  

## Computation of AUC for train data.  
```{r}
lr_AUC_train <- prediction(lr_prediction_train, mydata_train$label)
lr_AUC_train <- performance(lr_AUC_train, 'auc')
lr_AUC_train <- lr_AUC_train@y.values[[1]]
```

## Computation of TPR, FPR, TNR, FNR.  
```{r}
lr_TPR_train <- lr_TP_train / (lr_TP_train + lr_FN_train) * 100
lr_FPR_train <- lr_FP_train / (lr_FP_train + lr_TN_train) * 100
lr_FNR_train <- 100 - lr_TPR_train
lr_TNR_train <- 100 - lr_FPR_train
```




## Creating a vector (Algo, AUC, ACCURACY, TPR, FPR, TNR, FNR), for future use.  
```{r}
lr_AUC_ACCURACY_TPR_FPR_TNR_FNR_train <- c('LR_train', lr_AUC_train, round(lr_accuracy_train, 2), round(lr_TPR_train, 2), round(lr_FPR_train, 2), round(lr_TNR_train, 2), round(lr_FNR_train, 2))
lr_AUC_ACCURACY_TPR_FPR_TNR_FNR_train
```

# ============== Having computed for the train dataset, now we'll compute for the test data and do necessary computations.  


Ratio of BLACK to BLUE in response variable label in test data. The data is imbalanced.    
```{r}
table(mydata_test$label)
```


# Logistic Regression on testing dataset.  
```{r}
lr_prediction_test <- predict(lr_glm_model_train, newdata = mydata_test[,1:2], type = 'response')
lr_prediction_labels_test <- ifelse(lr_prediction_test > 0.5, "BLUE", "BLACK")
table(lr_prediction_labels_test)
```

```{r}
table(mydata_test$label)
```

## Computation of confusion matrix for test data, with table() function.  
```{r}
lr_confusion_matrix_test <- table(mydata_test$label, lr_prediction_labels_test)

# Gathering parts of confusion matrix, for later use:
lr_TP_test = lr_confusion_matrix_test[1, 1]
lr_FP_test = lr_confusion_matrix_test[1, 2]
lr_FN_test = lr_confusion_matrix_test[2, 1]
lr_TN_test = lr_confusion_matrix_test[2, 2]

lr_confusion_matrix_test
```




## Computation of Accuracy parameter from confusion matrix for test data.  
```{r}
lr_accuracy_test <- sum(diag(lr_confusion_matrix_test)) / sum(lr_confusion_matrix_test) * 100
lr_accuracy_test
```

## Verifying computation of confusion matrix for test data, with caret::confusionMatrix function.  
```{r}
caret::confusionMatrix(table(mydata_test$label, lr_prediction_labels_test))
```

So, with both methods Accuracy is 0.92 = 92%




Now, I'll compile together all the required information for test data i.e. AUC, ACCURACY, TPR, FPR, TNR, FNR.  

We have already computed Accuracy in lr_accuracy_test.  

## Computation of AUC for test data.  
```{r}
lr_AUC_test <- performance(prediction(lr_prediction_test, mydata_test$label), 'auc')
lr_AUC_test <- lr_AUC_test@y.values[[1]]
```

## Computation of TPR, FPR, TNR, FNR.  
```{r}
lr_TPR_test <- lr_TP_test / (lr_TP_test + lr_FN_test) * 100
lr_FPR_test <- lr_FP_test / (lr_FP_test + lr_TN_test) * 100
lr_FNR_test <- 100 - lr_TPR_test
lr_TNR_test <- 100 - lr_FPR_test
```

## Creating a vector (Algo, AUC, ACCURACY, TPR, FPR, TNR, FNR), for future use.  
```{r}
lr_AUC_ACCURACY_TPR_FPR_TNR_FNR_test <- c('LR_test', lr_AUC_test, round(lr_accuracy_test, 2), round(lr_TPR_test, 2), round(lr_FPR_test, 2), round(lr_TNR_test, 2), round(lr_FNR_test, 2))
lr_AUC_ACCURACY_TPR_FPR_TNR_FNR_test
```


# ============== Having computed for the LR algorithm, now we'll compute for the Naive Bayes.  


# Naive Bayes on entire dataset.  

## Executing naiveBayes().  
```{r}
nb_naiveBayes_model <- naiveBayes(label~., data = mydata)
nb_naiveBayes_model
```


```{r}
summary(nb_naiveBayes_model)
```


## Prediction and generation of Confusion Matrix.  
```{r}
nb_prediction <- predict(nb_naiveBayes_model, mydata)
nb_confusion_matrix <- table(nb_prediction, mydata$label)
nb_confusion_matrix
```



```{r}
nb_accuracy <- sum(diag(nb_confusion_matrix)) / sum(nb_confusion_matrix)
nb_accuracy
```

The Accuracy agrees with what we computed with LR.  

**Since this is greater than 70%, this again affirms the point that the model is not underfitting, and therefore capable of learning.**  


# Naive Bayes on training dataset.  
```{r}
nb_naiveBayes_model_train <- naiveBayes(label~., data = mydata_train)
nb_naiveBayes_model_train
```

```{r}
summary(nb_naiveBayes_model_train)
```

## Computation of confusion matrix for train data, with table() function.  
```{r}
nb_prediction_train <- predict(nb_naiveBayes_model_train, mydata_train)
nb_confusion_matrix_train <- table(nb_prediction_train, mydata_train$label)

# Gathering parts of confusion matrix, for later use:
nb_TP_train = nb_confusion_matrix_train[1, 1]
nb_FP_train = nb_confusion_matrix_train[1, 2]
nb_FN_train = nb_confusion_matrix_train[2, 1]
nb_TN_train = nb_confusion_matrix_train[2, 2]

nb_confusion_matrix_train
```


## Computation of Accuracy parameter from confusion matrix. 
```{r}
nb_accuracy_train <- sum(diag(nb_confusion_matrix_train)) / sum(nb_confusion_matrix_train)
nb_accuracy_train
```

## Computation of AUC for train data.  
```{r message= F}
nb_AUC_train <- roc(mydata_train$label, as.numeric(nb_prediction_train))
nb_AUC_train <- nb_AUC_train$auc
nb_AUC_train
```

## Computation of TPR, FPR, TNR, FNR.  
```{r}
nb_TPR_train <- nb_TP_train / (nb_TP_train + nb_FN_train) * 100
nb_FPR_train <- nb_FP_train / (nb_FP_train + nb_TN_train) * 100
nb_FNR_train <- 100 - nb_TPR_train
nb_TNR_train <- 100 - nb_FPR_train
```


## Creating a vector (Algo, AUC, ACCURACY, TPR, FPR, TNR, FNR), for future use.  
```{r}
nb_AUC_ACCURACY_TPR_FPR_TNR_FNR_train <- c('NB_train', nb_AUC_train, round(nb_accuracy_train, 2), round(nb_TPR_train, 2), round(nb_FPR_train, 2), round(nb_TNR_train, 2), round(nb_FNR_train, 2))
nb_AUC_ACCURACY_TPR_FPR_TNR_FNR_train
```

# ============== Having computed for the train dataset, now we'll compute for the test data and do necessary computations.  

# Naive Bayes on test dataset.  
```{r}
nb_prediction_test <- predict(nb_naiveBayes_model_train, mydata_test)
```


## Computation of confusion matrix for test data, with table() function.  
```{r}
nb_confusion_matrix_test <- table(nb_prediction_test, mydata_test$label)

# Gathering parts of confusion matrix, for later use:
nb_TP_test = nb_confusion_matrix_test[1, 1]
nb_FP_test = nb_confusion_matrix_test[1, 2]
nb_FN_test = nb_confusion_matrix_test[2, 1]
nb_TN_test = nb_confusion_matrix_test[2, 2]

nb_confusion_matrix_test
```


## Computation of Accuracy parameter from confusion matrix. 
```{r}
nb_accuracy_test <- sum(diag(nb_confusion_matrix_test)) / sum(nb_confusion_matrix_test)
nb_accuracy_test
```

## Computation of AUC for test data.  
```{r message= F}
library(pROC)
nb_AUC_test <- roc(mydata_test$label, as.numeric(nb_prediction_test))
nb_AUC_test <- nb_AUC_test$auc
nb_AUC_test
```

## Computation of TPR, FPR, TNR, FNR.  
```{r}
nb_TPR_test <- nb_TP_test / (nb_TP_test + nb_FN_test) * 100
nb_FPR_test <- nb_FP_test / (nb_FP_test + nb_TN_test) * 100
nb_FNR_test <- 100 - nb_TPR_test
nb_TNR_test <- 100 - nb_FPR_test
```

## Creating a vector (Algo, AUC, ACCURACY, TPR, FPR, TNR, FNR), for future use.  
```{r}
nb_AUC_ACCURACY_TPR_FPR_TNR_FNR_test <- c('NB_test', nb_AUC_test, round(nb_accuracy_test, 2), round(nb_TPR_test, 2), round(nb_FPR_test, 2), round(nb_TNR_test, 2), round(nb_FNR_test, 2))
nb_AUC_ACCURACY_TPR_FPR_TNR_FNR_test
```

# ============== Having computed for the NB algorithm, now we'll compute for the KNN.  

# KNN on entire training dataset.  
Since we ran LR and NB models on the enitre dataset, to determine whether the model can learn, we are skipping that step in KNN.   

## Executing KNN().  
Based on the requirement, first we'll do KNN on training data, with k = 3, and then k = 5.   
```{r}
knn_fit_train <- knn3(label~., data = mydata_train, k = 3)   # KNN with k = 3.
KNN_train <- predict(knn_fit_train, newdata = mydata_train, type = "class")
```


## Creating the confusion matrix.  
```{r}
knn_confusion_matrix_train <- table(KNN_train, mydata_train$label)

# Gathering parts of confusion matrix, for later use:
knn_TP_train = knn_confusion_matrix_train[1, 1]
knn_FP_train = knn_confusion_matrix_train[1, 2]
knn_FN_train = knn_confusion_matrix_train[2, 1]
knn_TN_train = knn_confusion_matrix_train[2, 2]

knn_confusion_matrix_train
```


## Computing accuracy.  
```{r}
knn_accuracy_train <- sum(diag(knn_confusion_matrix_train)) / sum(knn_confusion_matrix_train)
knn_accuracy_train
```

## Computation of AUC for train data.  
```{r message= F}
knn_AUC_train <- roc(mydata_train$label, as.numeric(KNN_train))
knn_AUC_train <- knn_AUC_train$auc
knn_AUC_train
```

## Computation of TPR, FPR, TNR, FNR.  
```{r}
knn_TPR_train <- knn_TP_train / (knn_TP_train + knn_FN_train) * 100
knn_FPR_train <- knn_FP_train / (knn_FP_train + knn_TN_train) * 100
knn_FNR_train <- 100 - knn_TPR_train
knn_TNR_train <- 100 - knn_FPR_train
```


## Creating a vector (Algo, AUC, ACCURACY, TPR, FPR, TNR, FNR), for future use.  
```{r}
knn_AUC_ACCURACY_TPR_FPR_TNR_FNR_train3 <- c('KNN_train3', knn_AUC_train, round(knn_accuracy_train, 2), round(knn_TPR_train, 2), round(knn_FPR_train, 2), round(knn_TNR_train, 2), round(knn_FNR_train, 2))
knn_AUC_ACCURACY_TPR_FPR_TNR_FNR_train3
```

# ============== Having computed for the train dataset, now we'll compute for the test data and do necessary computations.  

# KNN on entire testing dataset.  
Since we ran LR and NB models on the enitre dataset, to determine whether the model can learn, we are skipping that step in KNN.   

## Executing KNN().  
Based on the requirement, we'll do KNN on testing data, with k = 3.   
```{r}
knn_fit_test <- predict(knn_fit_train, newdata = mydata_test, type = "class")
```

## Creating the confusion matrix.  
```{r}
knn_confusion_matrix_test <- table(knn_fit_test, mydata_test$label)

# Gathering parts of confusion matrix, for later use:
knn_TP_test = knn_confusion_matrix_test[1, 1]
knn_FP_test = knn_confusion_matrix_test[1, 2]
knn_FN_test = knn_confusion_matrix_test[2, 1]
knn_TN_test = knn_confusion_matrix_test[2, 2]

knn_confusion_matrix_test
```

## Computing accuracy.  
```{r}
knn_accuracy_test <- sum(diag(knn_confusion_matrix_test)) / sum(knn_confusion_matrix_test)
knn_accuracy_test
```

## Computation of AUC for test data.  
```{r message= F}
knn_AUC_test <- roc(mydata_test$label, as.numeric(knn_fit_test))
knn_AUC_test <- knn_AUC_test$auc
knn_AUC_test
```

## Computation of TPR, FPR, TNR, FNR.  
```{r}
knn_TPR_test <- knn_TP_test / (knn_TP_test + knn_FN_test) * 100
knn_FPR_test <- knn_FP_test / (knn_FP_test + knn_TN_test) * 100
knn_FNR_test <- 100 - knn_TPR_test
knn_TNR_test <- 100 - knn_FPR_test
```


## Creating a vector (Algo, AUC, ACCURACY, TPR, FPR, TNR, FNR), for future use.  
```{r}
knn_AUC_ACCURACY_TPR_FPR_TNR_FNR_test3 <- c('KNN_test3', knn_AUC_test, round(knn_accuracy_test, 2), round(knn_TPR_test, 2), round(knn_FPR_test, 2), round(knn_TNR_test, 2), round(knn_FNR_test, 2))
knn_AUC_ACCURACY_TPR_FPR_TNR_FNR_test3
```

# ============== Having computed for the KNN = 3, now we'll compute for the KNN = 5.  


## Executing KNN().  
Based on the requirement, first we'll do KNN on training data, with k = 5.   
```{r}
knn_fit_train5 <- knn3(label~., data = mydata_train, k = 5)   # KNN with k = 5.
KNN_train5 <- predict(knn_fit_train5, newdata = mydata_train, type = "class")
```


## Creating the confusion matrix.  
```{r}
knn_confusion_matrix_train5 <- table(KNN_train5, mydata_train$label)

# Gathering parts of confusion matrix, for later use:
knn_TP_train5 = knn_confusion_matrix_train5[1, 1]
knn_FP_train5 = knn_confusion_matrix_train5[1, 2]
knn_FN_train5 = knn_confusion_matrix_train5[2, 1]
knn_TN_train5 = knn_confusion_matrix_train5[2, 2]

knn_confusion_matrix_train5
```


## Computing accuracy.  
```{r}
knn_accuracy_train5 <- sum(diag(knn_confusion_matrix_train5)) / sum(knn_confusion_matrix_train5)
knn_accuracy_train5
```

## Computation of AUC for train data.  
```{r message= F}
knn_AUC_train5 <- roc(mydata_train$label, as.numeric(KNN_train5))
knn_AUC_train5 <- knn_AUC_train5$auc
knn_AUC_train5
```

## Computation of TPR, FPR, TNR, FNR.  
```{r}
knn_TPR_train5 <- knn_TP_train5 / (knn_TP_train5 + knn_FN_train5) * 100
knn_FPR_train5 <- knn_FP_train5 / (knn_FP_train5 + knn_TN_train5) * 100
knn_FNR_train5 <- 100 - knn_TPR_train5
knn_TNR_train5 <- 100 - knn_FPR_train5
```


## Creating a vector (Algo, AUC, ACCURACY, TPR, FPR, TNR, FNR), for future use.  
```{r}
knn_AUC_ACCURACY_TPR_FPR_TNR_FNR_train5 <- c('KNN_train5', knn_AUC_train5, round(knn_accuracy_train5, 2), round(knn_TPR_train5, 2), round(knn_FPR_train5, 2), round(knn_TNR_train5, 2), round(knn_FNR_train5, 2))
knn_AUC_ACCURACY_TPR_FPR_TNR_FNR_train5
```

# ============== Having computed for the train dataset, now we'll compute for the test data and do necessary computations.  

# KNN on entire testing dataset.  
Since we ran LR and NB models on the enitre dataset, to determine whether the model can learn, we are skipping that step in KNN.   

## Executing KNN().  
Based on the requirement, we'll do KNN on testing data, with k = 5.   
```{r}
KNN_test5 <- predict(knn_fit_train5, newdata = mydata_test, type = "class")
```

```{r}
caret::confusionMatrix(KNN_test5, mydata_test$label)
```


## Creating the confusion matrix.  
```{r}
knn_confusion_matrix_test5 <- table(KNN_test5, mydata_test$label)

# Gathering parts of confusion matrix, for later use:
knn_TP_test5 = knn_confusion_matrix_test5[1, 1]
knn_FP_test5 = knn_confusion_matrix_test5[1, 2]
knn_FN_test5 = knn_confusion_matrix_test5[2, 1]
knn_TN_test5 = knn_confusion_matrix_test5[2, 2]

knn_confusion_matrix_test5
```

## Computing accuracy.  
```{r}
knn_accuracy_test5 <- sum(diag(knn_confusion_matrix_test5)) / sum(knn_confusion_matrix_test5)
knn_accuracy_test5
```

## Computation of AUC for test data.  
```{r message= F}
knn_AUC_test5 <- roc(mydata_test$label, as.numeric(KNN_test5))
knn_AUC_test5 <- knn_AUC_test5$auc
knn_AUC_test5
```

## Computation of TPR, FPR, TNR, FNR.  
```{r}
knn_TPR_test5 <- knn_TP_test5 / (knn_TP_test5 + knn_FN_test5) * 100
knn_FPR_test5 <- knn_FP_test5 / (knn_FP_test5 + knn_TN_test5) * 100
knn_FNR_test5 <- 100 - knn_TPR_test5
knn_TNR_test5 <- 100 - knn_FPR_test5
```


## Creating a vector (Algo, AUC, ACCURACY, TPR, FPR, TNR, FNR), for future use.  
```{r}
knn_AUC_ACCURACY_TPR_FPR_TNR_FNR_test5 <- c('KNN_test5', knn_AUC_test5, round(knn_accuracy_test5, 2), round(knn_TPR_test5, 2), round(knn_FPR_test5, 2), round(knn_TNR_test5, 2), round(knn_FNR_test5, 2))
knn_AUC_ACCURACY_TPR_FPR_TNR_FNR_test5
```

# ===========================================  

# Final answers to the Homework 1 requirements.  

1) The two required tables are shown at the bottom. The first table is the learnability table (based on training data) and the second table is the generalizability table (based test data). The former shows the model's ability to learn and the latter shows its ability to generalize.   

2) "1) Help your client understand that you have selected an appropriate model that has the capacity to learn".  
   I built the models for entire dataset with Logistic Regression (LR), Naive Bayes (NB). In both cases the readings of Accuracy parameter were greater than 70%. So, the models are not underfitting. So, based on class lectures and notes, I conclude that the models are capable of learning.  

3) "2) Help your client understand that when deployed your model is capable of generalizing".  
   All the models LR, NB and KNN performed very well on the training dataset, keeping an Accuracy of 100% in the first, 92% in the sedcond and 92% and 88% in two KNNs. However, none of the three models LR, NB, KNN (with k == 3 and k == 5) were able to generalize.  
   
   The Accuracy of LR deteroriated from 100% to 70%, NB deteroriated from 92% to 60%, KNN (k == 3) deteroriated from 85% to 70% and KNN (k == 5) deteroriated from 88% to 70%. The relative deterioration was least in KNN (k == 3), which was (85 - 70) / 85 * 100 = 17.64%.  
   
   So, overall, I would think that KNN (k == 3) was least bad. So, if I have to, then I would recommend this to my client.   
   
   However, a scanty data of 36 records and 2 independent column is not a realistic situation. So, I would request my client for a bigger dataset, including a good metadata.  
   
   Furthermore, I have seen KNN (k == 3) to perform erratically, yielding close, but different values (confusion matrix, accuracy, AUC etc) on different runs, the data and seed value for datasplit remaining constant.  
   
 
```{r}
table_1 <- data.frame(matrix(ncol = 6, nrow = 0))
table_1 <- rbind(table_1, lr_AUC_ACCURACY_TPR_FPR_TNR_FNR_train, nb_AUC_ACCURACY_TPR_FPR_TNR_FNR_train, knn_AUC_ACCURACY_TPR_FPR_TNR_FNR_train3, knn_AUC_ACCURACY_TPR_FPR_TNR_FNR_train5)
colnames(table_1) <- c("ALGO", "AUC","ACCURACY", "TPR", "FPR", "TNR", "FNR")

print("Learning table:")
table_1
```


```{r}
table_2 <- data.frame(matrix(ncol = 6, nrow = 0))
table_2 <- rbind(table_2, lr_AUC_ACCURACY_TPR_FPR_TNR_FNR_test, nb_AUC_ACCURACY_TPR_FPR_TNR_FNR_test, knn_AUC_ACCURACY_TPR_FPR_TNR_FNR_test3, knn_AUC_ACCURACY_TPR_FPR_TNR_FNR_test5)
colnames(table_2) <- c("ALGO", "AUC","ACCURACY", "TPR", "FPR", "TNR", "FNR")

print("Generalizing table:")
table_2
```


